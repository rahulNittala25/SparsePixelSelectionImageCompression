{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a49086",
   "metadata": {},
   "source": [
    "## 1. IMPORT RELEVANT LIBRARIES\n",
    "\n",
    "Please have a look at the **requirements.txt** file for the prerequisite libraries and their versions.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58dd2b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b91b43",
   "metadata": {},
   "source": [
    "## 2. CREATE THE ARCHITECTURES FOR MASK GENERATOR AND INPAINTING GENRATOR\n",
    "\n",
    "* The **Binarization layer** works as follows:<br> In *forward propagation*, rounds each element to the nearest integer.<br>In *backward propagation*, sends back the gradients as is.\n",
    "\n",
    "* Both the Mask Generator(*Mask_Generator_Net*) and the Inpainting Generator(*Generator_Net*) follow the same architectuer as prescribed in the paper except the first Conv Block and the last Transposed Conv block. Both of them have 64 final channels rather than 128.\n",
    "\n",
    "**NOTE:** Both these networks are saved in the **Generator.py** file within the ./Module folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b4d8899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.autograd\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "class Binarization(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,input):\n",
    "        return torch.round(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        return grad_output\n",
    "\n",
    "class Mask_Generator_Net(nn.Module):\n",
    "    def __init__(self, in_channels=6):\n",
    "        super().__init__()\n",
    "        ###### C-Block Stacking ##########\n",
    "        self.l11 = nn.Conv2d(in_channels, 32, 5, padding='same')\n",
    "        self.l12 = nn.Conv2d(in_channels, 16, 5, padding='same', dilation=2)\n",
    "        self.l13 = nn.Conv2d(in_channels, 16, 5, padding='same', dilation=5)\n",
    "        self.d1 = nn.MaxPool2d(2)\n",
    "        self.l21 = nn.Conv2d(64, 64, 5, padding='same')\n",
    "        self.l22 = nn.Conv2d(64, 32, 5, padding='same', dilation=2)\n",
    "        self.l23 = nn.Conv2d(64, 32, 5, padding='same', dilation=5)\n",
    "        self.d2 = nn.MaxPool2d(2)\n",
    "        self.l31 = nn.Conv2d(128, 128, 5, padding='same')\n",
    "        self.l32 = nn.Conv2d(128, 64, 5, padding='same', dilation=2)\n",
    "        self.l33 = nn.Conv2d(128, 64, 5, padding='same', dilation=5)\n",
    "        self.d3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        \n",
    "        self.l41 = nn.Conv2d(256, 256, 5, padding='same')\n",
    "        self.l42 = nn.Conv2d(256, 128, 5, padding='same', dilation=2)\n",
    "        self.l43 = nn.Conv2d(256, 128, 5, padding='same', dilation=5)\n",
    "        \n",
    "        \n",
    "        self.u1 = nn.Upsample(scale_factor=2)\n",
    "        self.tl11 = nn.ConvTranspose2d(256+512, 128, 5, padding=2)\n",
    "        self.tl12 = nn.ConvTranspose2d(256+512, 64, 5, dilation=2, padding=4)\n",
    "        self.tl13 = nn.ConvTranspose2d(256+512, 64, 5, dilation=5, padding=10)\n",
    "        self.u2 = nn.Upsample(scale_factor=2)\n",
    "        self.tl21 = nn.ConvTranspose2d(128+256, 64, 5, padding=2)\n",
    "        self.tl22 = nn.ConvTranspose2d(128+256, 32, 5, padding=4, dilation=2)\n",
    "        self.tl23 = nn.ConvTranspose2d(128+256, 32, 5, padding=10, dilation=5)\n",
    "        self.u3 = nn.Upsample(scale_factor=2)\n",
    "        self.tl31 = nn.ConvTranspose2d(64+128, 32, 5, padding=2)\n",
    "        self.tl32 = nn.ConvTranspose2d(64+128, 16, 5, padding=4, dilation=2)\n",
    "        self.tl33 = nn.ConvTranspose2d(64+128, 16, 5, padding=10, dilation=5)\n",
    "        ###### Conv Layer Stacking ##########\n",
    "        self.conv1 = nn.ConvTranspose2d(64+in_channels,8, 3, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(8, 1, 3, padding=1)\n",
    "        self.mask = Binarization.apply\n",
    "\n",
    "    def forward(self, x):\n",
    "        #### Conv Block 1\n",
    "        temp1 = F.elu(self.l11(x))\n",
    "        temp2 = F.elu(self.l12(x))\n",
    "        temp3 = F.elu(self.l13(x))\n",
    "        x1o = torch.cat([temp1,temp2, temp3], dim=1)\n",
    "        out = self.d1(x1o)\n",
    "        \n",
    "        #### Conv Block 2\n",
    "        temp1 = F.elu(self.l21(out))\n",
    "        temp2 = F.elu(self.l22(out))\n",
    "        temp3 = F.elu(self.l23(out))\n",
    "        x2o = torch.cat([temp1,temp2, temp3], dim=1)\n",
    "        out = self.d2(x2o)\n",
    "        \n",
    "        #### Conv Block 3\n",
    "        temp1 = F.elu(self.l31(out))\n",
    "        temp2 = F.elu(self.l32(out))\n",
    "        temp3 = F.elu(self.l33(out))\n",
    "        x3o = torch.cat([temp1,temp2, temp3], dim=1)\n",
    "        out = self.d3(x3o)\n",
    "\n",
    "        #### Conv Block 3\n",
    "        temp1 = F.elu(self.l41(out))\n",
    "        temp2 = F.elu(self.l42(out))\n",
    "        temp3 = F.elu(self.l43(out))\n",
    "        x4o = torch.cat([temp1,temp2, temp3], dim=1)\n",
    "        out = self.u1(x4o) \n",
    "        \n",
    "        #### Tr-Conv Block 2\n",
    "        out = torch.cat([out, x3o], dim=1)\n",
    "        temp1 = F.elu(self.tl11(out))\n",
    "        temp2 = F.elu(self.tl12(out))\n",
    "        temp3 = F.elu(self.tl13(out))\n",
    "        out = torch.cat([temp1, temp2, temp3], dim=1)\n",
    "        out = self.u2(out)\n",
    "        \n",
    "        #### Tr-Conv Block 3\n",
    "        out = torch.cat([out, x2o], dim=1)\n",
    "        temp1 = F.elu(self.tl21(out))\n",
    "        temp2 = F.elu(self.tl22(out))\n",
    "        temp3 = F.elu(self.tl23(out))\n",
    "        out = torch.cat([temp1, temp2, temp3], dim=1)\n",
    "        out = self.u3(out)\n",
    "        \n",
    "        #### Tr-Conv Block 4\n",
    "        out = torch.cat([out, x1o], dim=1)\n",
    "        temp1 = F.elu(self.tl31(out))\n",
    "        temp2 = F.elu(self.tl32(out))\n",
    "        temp3 = F.elu(self.tl33(out))\n",
    "        out = torch.cat([temp1, temp2, temp3, x], dim=1)\n",
    "        \n",
    "        #### Conv Layer Head 1\n",
    "        out = F.elu(self.conv1(out))\n",
    "        \n",
    "        out = F.hardsigmoid(self.conv2(out))\n",
    "        \n",
    "        out = self.mask(out)\n",
    "        return out\n",
    "\n",
    "class Generator_Net(nn.Module):\n",
    "    def __init__(self, in_channels=7):\n",
    "        super().__init__()\n",
    "        ###### C-Block Stacking ##########\n",
    "        self.l11 = nn.Conv2d(in_channels, 32, 5, padding='same')\n",
    "        self.l12 = nn.Conv2d(in_channels, 16, 5, padding='same', dilation=2)\n",
    "        self.l13 = nn.Conv2d(in_channels, 16, 5, padding='same', dilation=5)\n",
    "        self.d1 = nn.MaxPool2d(2)\n",
    "        self.l21 = nn.Conv2d(64, 64, 5, padding='same')\n",
    "        self.l22 = nn.Conv2d(64, 32, 5, padding='same', dilation=2)\n",
    "        self.l23 = nn.Conv2d(64, 32, 5, padding='same', dilation=5)\n",
    "        self.d2 = nn.MaxPool2d(2)\n",
    "        self.l31 = nn.Conv2d(128, 128, 5, padding='same')\n",
    "        self.l32 = nn.Conv2d(128, 64, 5, padding='same', dilation=2)\n",
    "        self.l33 = nn.Conv2d(128, 64, 5, padding='same', dilation=5)\n",
    "        self.d3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        \n",
    "        self.l41 = nn.Conv2d(256, 256, 5, padding='same')\n",
    "        self.l42 = nn.Conv2d(256, 128, 5, padding='same', dilation=2)\n",
    "        self.l43 = nn.Conv2d(256, 128, 5, padding='same', dilation=5)\n",
    "        \n",
    "        \n",
    "        self.u1 = nn.Upsample(scale_factor=2)\n",
    "        self.tl11 = nn.ConvTranspose2d(256+512, 128, 5, padding=2)\n",
    "        self.tl12 = nn.ConvTranspose2d(256+512, 64, 5, dilation=2, padding=4)\n",
    "        self.tl13 = nn.ConvTranspose2d(256+512, 64, 5, dilation=5, padding=10)\n",
    "        self.u2 = nn.Upsample(scale_factor=2)\n",
    "        self.tl21 = nn.ConvTranspose2d(128+256, 64, 5, padding=2)\n",
    "        self.tl22 = nn.ConvTranspose2d(128+256, 32, 5, padding=4, dilation=2)\n",
    "        self.tl23 = nn.ConvTranspose2d(128+256, 32, 5, padding=10, dilation=5)\n",
    "        self.u3 = nn.Upsample(scale_factor=2)\n",
    "        self.tl31 = nn.ConvTranspose2d(64+128, 32, 5, padding=2)\n",
    "        self.tl32 = nn.ConvTranspose2d(64+128, 16, 5, padding=4, dilation=2)\n",
    "        self.tl33 = nn.ConvTranspose2d(64+128, 16, 5, padding=10, dilation=5)\n",
    "        ###### Conv Layer Stacking ##########\n",
    "        self.conv1 = nn.ConvTranspose2d(64+in_channels,8, 3, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(8, 3, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #### Conv Block 1\n",
    "        temp1 = F.elu(self.l11(x))\n",
    "        temp2 = F.elu(self.l12(x))\n",
    "        temp3 = F.elu(self.l13(x))\n",
    "        x1o = torch.cat([temp1,temp2, temp3], dim=1)\n",
    "        out = self.d1(x1o)\n",
    "        \n",
    "        #### Conv Block 2\n",
    "        temp1 = F.elu(self.l21(out))\n",
    "        temp2 = F.elu(self.l22(out))\n",
    "        temp3 = F.elu(self.l23(out))\n",
    "        x2o = torch.cat([temp1,temp2, temp3], dim=1)\n",
    "        out = self.d2(x2o)\n",
    "        \n",
    "        #### Conv Block 3\n",
    "        temp1 = F.elu(self.l31(out))\n",
    "        temp2 = F.elu(self.l32(out))\n",
    "        temp3 = F.elu(self.l33(out))\n",
    "        x3o = torch.cat([temp1,temp2, temp3], dim=1)\n",
    "        out = self.d3(x3o)\n",
    "\n",
    "        #### Conv Block 3\n",
    "        temp1 = F.elu(self.l41(out))\n",
    "        temp2 = F.elu(self.l42(out))\n",
    "        temp3 = F.elu(self.l43(out))\n",
    "        x4o = torch.cat([temp1,temp2, temp3], dim=1)\n",
    "        out = self.u1(x4o) \n",
    "        \n",
    "        #### Tr-Conv Block 2\n",
    "        out = torch.cat([out, x3o], dim=1)\n",
    "        temp1 = F.elu(self.tl11(out))\n",
    "        temp2 = F.elu(self.tl12(out))\n",
    "        temp3 = F.elu(self.tl13(out))\n",
    "        out = torch.cat([temp1, temp2, temp3], dim=1)\n",
    "        out = self.u2(out)\n",
    "        \n",
    "        #### Tr-Conv Block 3\n",
    "        out = torch.cat([out, x2o], dim=1)\n",
    "        temp1 = F.elu(self.tl21(out))\n",
    "        temp2 = F.elu(self.tl22(out))\n",
    "        temp3 = F.elu(self.tl23(out))\n",
    "        out = torch.cat([temp1, temp2, temp3], dim=1)\n",
    "        out = self.u3(out)\n",
    "        \n",
    "        #### Tr-Conv Block 4\n",
    "        out = torch.cat([out, x1o], dim=1)\n",
    "        temp1 = F.elu(self.tl31(out))\n",
    "        temp2 = F.elu(self.tl32(out))\n",
    "        temp3 = F.elu(self.tl33(out))\n",
    "        out = torch.cat([temp1, temp2, temp3, x], dim=1)\n",
    "        \n",
    "        #### Conv Layer Head 1\n",
    "        out = F.elu(self.conv1(out))\n",
    "        \n",
    "        out = F.hardsigmoid(self.conv2(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e16e50a",
   "metadata": {},
   "source": [
    "## 3. CREATE THE ARCHITECTURE FOR DISCRIMINATOR\n",
    "\n",
    "WGAN network was prescribed in the paper for stabilising the GAN training. There are multiple ways of doing this. In this implementation I used Spectral Norm as the way to enfore the Lipschitz continuity.\n",
    "\n",
    "**THE CODE FOR THE SPECTRAL NORM IS BORROWED FROM** [DVD-GAN repository by Harrypotterrrr](https://github.com/Harrypotterrrr/DVD-GAN).<br>\n",
    "\n",
    "**Architecture details:** I used a funnel network with 4 conv blocks. Each block contains a conv layer, whose weights are normalized using spectral norm, followed by a Leaky ReLU activation and a downsampling with factor 2. We used 64,128,256,512 channels respectively for each conv block. We then flatten the embedding and used a fully connected layer to get the discriminator score.\n",
    "\n",
    "**NOTE:** The network is saved in the **Discriminators.py** file within the ./Module folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ad25556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import init\n",
    "\n",
    "from Module.Normalization import  SpectralNorm\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=4):\n",
    "        super().__init__()\n",
    "        self.pre_conv = nn.Sequential(SpectralNorm(nn.Conv2d(in_channels,64,3, padding='same')),\n",
    "                                      nn.LeakyReLU(),\n",
    "                                      nn.MaxPool2d(2),\n",
    "                                       SpectralNorm(nn.Conv2d(64,128,3, padding='same')), \n",
    "                                       nn.LeakyReLU(),\n",
    "                                      nn.MaxPool2d(2),\n",
    "                                      SpectralNorm(nn.Conv2d(128,256,3, padding='same')),\n",
    "                                       nn.LeakyReLU(),\n",
    "                                      nn.MaxPool2d(2),\n",
    "                                      SpectralNorm(nn.Conv2d(256,512,3, padding='same')),\n",
    "                                       nn.LeakyReLU(),\n",
    "                                      nn.MaxPool2d(2)\n",
    "                                      )\n",
    "        self.linear = SpectralNorm(nn.Linear(512*8*8, 1))\n",
    "    def forward(self,x):\n",
    "        out = self.pre_conv(x)\n",
    "        B,C,H,W = out.shape\n",
    "        out = out.view(B,-1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a6187",
   "metadata": {},
   "source": [
    "## 4. REUSABLE UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd2688d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wasserstein Generator Loss\n",
    "def wass_g_loss(fake_score):\n",
    "    return torch.mean(fake_score)\n",
    "\n",
    "# Wasserstein Disciminator Loss\n",
    "def wass_d_loss(real_score, fake_score):\n",
    "    return -1.0*torch.mean(fake_score) + torch.mean(real_score)\n",
    "\n",
    "# Mask density loss to encourage selecting only wanted_density% of the total pixels. L2 loss\n",
    "def mask_density_loss(mask, wanted_density):    \n",
    "    density = torch.div(torch.sum(mask), (mask.shape[0]*128*128))\n",
    "    return torch.pow((density - wanted_density), 2)\n",
    "    \n",
    "\n",
    "# Image reconstruction Loss. L1 loss used in accordance with the paper.\n",
    "def image_loss(real_image, fake_image):\n",
    "    return F.l1_loss(fake_image, real_image, reduce='mean')\n",
    "\n",
    "# Write TensorboardX logs for full joint training scenario\n",
    "def write_log_full(writer, log_str, step, d_loss_real, d_loss_fake, ds_loss, density_loss, image_loss):\n",
    "\n",
    "    writer.add_scalar('data/ds_loss_real', d_loss_real.item(), step)\n",
    "    writer.add_scalar('data/ds_loss_fake', d_loss_fake.item(), step)\n",
    "    writer.add_scalar('data/ds_loss', ds_loss.item(), step)\n",
    "    writer.add_scalar('data/density_loss', density_loss.item(), step)\n",
    "    writer.add_scalar('data/image_reconstruction_loss', image_loss.item(), step)\n",
    "\n",
    "    writer.add_text('logs', log_str, step)\n",
    "    \n",
    "# Write TensorboardX logs for No mask network scenario\n",
    "def write_log_no_mask(writer, log_str, step, d_loss_real, d_loss_fake, ds_loss, image_loss):\n",
    "\n",
    "    writer.add_scalar('data/ds_loss_real', d_loss_real.item(), step)\n",
    "    writer.add_scalar('data/ds_loss_fake', d_loss_fake.item(), step)\n",
    "    writer.add_scalar('data/ds_loss', ds_loss.item(), step)\n",
    "    writer.add_scalar('data/image_reconstruction_loss', image_loss.item(), step)\n",
    "\n",
    "    writer.add_text('logs', log_str, step)\n",
    "    \n",
    "# Set target device/devices for training based on availability.\n",
    "def set_device(config):\n",
    "\n",
    "    if config.gpus == \"\": # cpu\n",
    "        return 'cpu', False, \"\"\n",
    "    else:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(config.gpus)\n",
    "\n",
    "        if torch.cuda.is_available() is False: # cpu\n",
    "            return 'cpu', False, \"\"\n",
    "        else:\n",
    "            gpus = list(range(len(config.gpus)))\n",
    "            if config.parallel is True and len(gpus) > 1: # multi gpus\n",
    "                return 'cuda:0', True, gpus\n",
    "            else:\n",
    "                return 'cuda:'+ str(gpus[0]), False, gpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1fbe3",
   "metadata": {},
   "source": [
    "## 5. TRAINER CLASSES FOR DIFFERENT TRAINING SCENARIOS\n",
    "\n",
    "For the sake of experimentation we are training models for two different scenarios.\n",
    "1. **Full Model**: All models(Mask_Generator_Net + Geneator_Net + Discriminator) are jointly trained. **Trainer** class\n",
    "2. **No Mask N/w**: No Mask_Generator_Net. Instead, masks of target density are sampled. The Generator_Net and Discriminator used normally. **Trainer_NoMask** class\n",
    "\n",
    "The structure of both the Trainer classes is motivated by trainer.py file of [DVD-GAN repository by Harrypotterrrr](https://github.com/Harrypotterrrr/DVD-GAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c3829fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, data_loader, config):\n",
    "        '''Initialize the hyperparameters and paths for training. Initialize models/load pretrained models and create optimizers'''\n",
    "\n",
    "        # Data loader\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "        self.total_epoch = config.total_epoch\n",
    "        self.batch_size = config.batch_size\n",
    "        self.num_workers = config.num_workers\n",
    "        self.g_lr = config.g_lr\n",
    "        self.d_lr = config.d_lr\n",
    "        self.pretrained_model = config.pretrained_model\n",
    "\n",
    "        self.use_tensorboard = config.use_tensorboard\n",
    "        self.density = config.density\n",
    "        self.d_iters = config.d_iters\n",
    "\n",
    "        # path\n",
    "        self.log_path = config.log_path\n",
    "        self.model_save_path = config.model_save_path\n",
    "        self.sample_path = config.sample_path\n",
    "\n",
    "        # epoch size\n",
    "        self.log_epoch = config.log_epoch\n",
    "        self.sample_epoch = config.sample_epoch\n",
    "        self.model_save_epoch = config.model_save_epoch\n",
    "        self.version = config.version\n",
    "\n",
    "        # Path\n",
    "        self.log_path = os.path.join(config.log_path, self.version)\n",
    "        self.sample_path = os.path.join(config.sample_path, self.version)\n",
    "        self.model_save_path = os.path.join(config.model_save_path, self.version)\n",
    "\n",
    "        self.device, self.parallel, self.gpus = set_device(config)\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "        if self.use_tensorboard:\n",
    "            self.build_tensorboard()\n",
    "\n",
    "        # Start with trained model\n",
    "        if self.pretrained_model:\n",
    "            print('load_pretrained_model...')\n",
    "            self.load_pretrained_model()\n",
    "\n",
    "\n",
    "    def select_opt_schr(self):\n",
    "        '''Initialize the optimizers for the two Generator networks and the Discriminator network'''\n",
    "\n",
    "        self.g_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, list(self.G.parameters())+list(self.mask_G.parameters())), self.g_lr,\n",
    "                                            eps=1e-07)\n",
    "        self.d_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.D.parameters()), self.d_lr,\n",
    "                                            eps=1e-07)\n",
    "\n",
    "    def epoch2step(self):\n",
    "        '''Convert epochs into number of steps based on the size of the dataloader'''\n",
    "\n",
    "        self.epoch = 0\n",
    "        step_per_epoch = len(self.data_loader)\n",
    "        print(\"steps per epoch:\", step_per_epoch)\n",
    "\n",
    "        self.total_step = self.total_epoch * step_per_epoch\n",
    "        self.log_step = self.log_epoch * step_per_epoch\n",
    "        self.sample_step = self.sample_epoch * step_per_epoch\n",
    "        self.model_save_step = self.model_save_epoch * step_per_epoch\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # Data iterator\n",
    "        print(\"Inside the trainer!\")\n",
    "        data_iter = iter(self.data_loader)\n",
    "        print(\"Iterator created!\")\n",
    "        self.epoch2step()\n",
    "\n",
    "        # Start with trained model\n",
    "        if self.pretrained_model:\n",
    "            start = self.pretrained_model + 1\n",
    "        else:\n",
    "            start = 1\n",
    "\n",
    "        # Start time\n",
    "        print(\"=\" * 30, \"\\nStart training...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.D.train()\n",
    "        self.G.train()\n",
    "        self.mask_G.train()\n",
    "\n",
    "        for step in range(start, self.total_step+1):\n",
    "\n",
    "            try:\n",
    "                real_imgs, _ = next(data_iter)\n",
    "            except:\n",
    "                data_iter = iter(self.data_loader)\n",
    "                real_imgs, _ = next(data_iter)\n",
    "                self.epoch += 1\n",
    "\n",
    "            real_imgs = real_imgs.to(self.device)\n",
    "\n",
    "            # ================ update D d_iters times ================ #\n",
    "            for i in range(self.d_iters):\n",
    "                \n",
    "                # ============== Genrate Masks =================== #\n",
    "                img_masks = self.mask_G(torch.cat((real_imgs, torch.normal(0.0,0.1,size=real_imgs.size()).to(device)), 1))\n",
    "                \n",
    "                # ============== Create Masked images ============== #\n",
    "                \n",
    "                masked_imgs = img_masks * real_imgs\n",
    "                \n",
    "                # ============== Generator - Image reconstruction ================= #\n",
    "                \n",
    "                fake_imgs = self.G(torch.cat((masked_imgs, img_masks, torch.normal(0.0,0.1,size=real_imgs.size()).to(device)), 1))              \n",
    "                #print(\"Inpainted Video Generated\")\n",
    "                \n",
    "                fake_input = torch.cat((fake_imgs, img_masks), 1)\n",
    "                real_input = torch.cat((real_imgs, img_masks), 1)\n",
    "\n",
    "\n",
    "        # ============== Calculate losses and update the networks =========== #                \n",
    "                    \n",
    "                d_fake = self.D(fake_input)\n",
    "\n",
    "                # ============  Update the two Generator networks (Wasserstein Generator loss + Image Reconstruction loss + Mask Density loss)\n",
    "                g_loss = 0.005*wass_g_loss(d_fake)\n",
    "                mse_loss = image_loss(fake_imgs, real_imgs)\n",
    "                density_loss = mask_density_loss(img_masks, self.density)\n",
    "\n",
    "                g_combined_loss = g_loss + mse_loss + 100* density_loss\n",
    "                self.reset_grad()\n",
    "                g_combined_loss.backward()\n",
    "                self.g_optimizer.step()\n",
    "\n",
    "                # ============ Update the Discriminator network (Wasserstein Discriminator loss)\n",
    "                d_fake = self.D(fake_input.detach())\n",
    "                d_real = self.D(real_input.detach())\n",
    "                d_loss = wass_d_loss(d_real, d_fake)\n",
    "                self.reset_grad()\n",
    "                d_loss.backward()\n",
    "                self.d_optimizer.step()\n",
    "\n",
    "                \n",
    "            # ==================== print & save part ==================== #\n",
    "            # Print out log info\n",
    "            if step % self.log_step == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "                start_time = time.time()\n",
    "                log_str = \"Epoch: [%d/%d], Step: [%d/%d], time: %s, d_real: %.4f, d_fake: %.4f, d_loss: %.4f, g_loss: %.4f, density_loss: %.4f, image_loss: %.4f\" % \\\n",
    "                    (self.epoch+1, self.total_epoch, step, self.total_step, elapsed, torch.mean(d_real), torch.mean(d_fake), d_loss, g_loss, density_loss, mse_loss)\n",
    "\n",
    "                if self.use_tensorboard is True:\n",
    "                    write_log_full(self.writer, log_str, step, torch.mean(d_real), torch.mean(d_fake), d_loss, density_loss, mse_loss)\n",
    "                print(log_str)\n",
    "\n",
    "            # Save model\n",
    "            if step % self.model_save_step == 0:\n",
    "                torch.save(self.G.state_dict(),\n",
    "                           os.path.join(self.model_save_path, '{}_G.pth'.format(step)))\n",
    "                torch.save(self.mask_G.state_dict(),\n",
    "                           os.path.join(self.model_save_path, '{}_mask_G.pth'.format(step)))\n",
    "                torch.save(self.D.state_dict(),\n",
    "                           os.path.join(self.model_save_path, '{}_D.pth'.format(step)))\n",
    "\n",
    "    def build_model(self):\n",
    "        '''Initialize all three networks along with their optimizers and load them onto relevant device'''\n",
    "\n",
    "        print(\"=\" * 30, '\\nBuild_model...')\n",
    "        self.mask_G = Mask_Generator_Net(6).to(self.device)\n",
    "        self.G = Generator_Net(7).to(self.device)\n",
    "        self.D = Discriminator(4).to(self.device)\n",
    "\n",
    "        if self.parallel:\n",
    "            print('Use parallel...')\n",
    "            print('gpus:', os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "            self.mask_G = nn.DataParallel(self.mask_G, device_ids=self.gpus)\n",
    "            self.G = nn.DataParallel(self.G, device_ids=self.gpus)\n",
    "            self.D = nn.DataParallel(self.D, device_ids=self.gpus)\n",
    "\n",
    "        self.select_opt_schr()\n",
    "        print(\"Model building done!\")\n",
    "\n",
    "    def build_tensorboard(self):\n",
    "        '''Initialize TensorboardX summary writer'''\n",
    "        \n",
    "        from tensorboardX import SummaryWriter\n",
    "        self.writer = SummaryWriter(log_dir=self.log_path)\n",
    "\n",
    "    def load_pretrained_model(self):\n",
    "        '''Load pretrained models of all three networks from self.model_save_path folder'''\n",
    "        \n",
    "        self.G.load_state_dict(torch.load(os.path.join(\n",
    "            self.model_save_path, '{}_G.pth'.format(self.pretrained_model))))\n",
    "        self.mask_G.load_state_dict(torch.load(os.path.join(\n",
    "            self.model_save_path, '{}_mask_G.pth'.format(self.pretrained_model))))\n",
    "        self.D.load_state_dict(torch.load(os.path.join(\n",
    "            self.model_save_path, '{}_D.pth'.format(self.pretrained_model))))\n",
    "        print('loaded trained models (step: {})..!'.format(self.pretrained_model))\n",
    "\n",
    "    def reset_grad(self):\n",
    "        '''Reset the gradients before backward propagation of losses'''\n",
    "        \n",
    "        self.d_optimizer.zero_grad()\n",
    "        self.g_optimizer.zero_grad()\n",
    "\n",
    "    def save_sample(self, data_iter):\n",
    "        '''Save relevant samples at the current step'''\n",
    "        \n",
    "        real_images, _ = next(data_iter)\n",
    "        save_image(real_images, os.path.join(self.sample_path, 'real.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bab5cc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "class Trainer_NoMask(object):\n",
    "    def __init__(self, data_loader, config):\n",
    "        '''Initialize the hyperparameters and paths for training. Initialize models/load pretrained models and create optimizers'''\n",
    "\n",
    "        # Data loader\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "        self.total_epoch = config.total_epoch\n",
    "        self.batch_size = config.batch_size\n",
    "        self.num_workers = config.num_workers\n",
    "        self.g_lr = config.g_lr\n",
    "        self.d_lr = config.d_lr\n",
    "        self.pretrained_model = config.pretrained_model\n",
    "\n",
    "        self.use_tensorboard = config.use_tensorboard\n",
    "        self.density = config.density\n",
    "        self.d_iters = config.d_iters\n",
    "\n",
    "        # path\n",
    "        self.log_path = config.log_path\n",
    "        self.model_save_path = config.model_save_path\n",
    "        self.sample_path = config.sample_path\n",
    "\n",
    "        # epoch size\n",
    "        self.log_epoch = config.log_epoch\n",
    "        self.sample_epoch = config.sample_epoch\n",
    "        self.model_save_epoch = config.model_save_epoch\n",
    "        self.version = config.version\n",
    "\n",
    "        # Path\n",
    "        self.log_path = os.path.join(config.log_path, self.version)\n",
    "        self.sample_path = os.path.join(config.sample_path, self.version)\n",
    "        self.model_save_path = os.path.join(config.model_save_path, self.version)\n",
    "\n",
    "        self.device, self.parallel, self.gpus = set_device(config)\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "        if self.use_tensorboard:\n",
    "            self.build_tensorboard()\n",
    "\n",
    "        # Start with trained model\n",
    "        if self.pretrained_model:\n",
    "            print('load_pretrained_model...')\n",
    "            self.load_pretrained_model()\n",
    "\n",
    "\n",
    "    def select_opt_schr(self):\n",
    "        '''Initialize the optimizers for the Generator network(no Mask_Generator_Net) and the Discriminator network'''\n",
    "\n",
    "        self.g_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.G.parameters()), self.g_lr,\n",
    "                                            eps=1e-07)\n",
    "        self.d_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.D.parameters()), self.d_lr,\n",
    "                                            eps=1e-07)\n",
    "\n",
    "    def epoch2step(self):\n",
    "        '''Convert epochs into number of steps based on the size of the dataloader'''\n",
    "\n",
    "        self.epoch = 0\n",
    "        step_per_epoch = len(self.data_loader)\n",
    "        print(\"steps per epoch:\", step_per_epoch)\n",
    "\n",
    "        self.total_step = self.total_epoch * step_per_epoch\n",
    "        self.log_step = self.log_epoch * step_per_epoch\n",
    "        self.sample_step = self.sample_epoch * step_per_epoch\n",
    "        self.model_save_step = self.model_save_epoch * step_per_epoch\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # Data iterator\n",
    "        print(\"Inside the trainer!\")\n",
    "        data_iter = iter(self.data_loader)\n",
    "        print(\"Iterator created!\")\n",
    "        self.epoch2step()\n",
    "\n",
    "        # Start with trained model\n",
    "        if self.pretrained_model:\n",
    "            start = self.pretrained_model + 1\n",
    "        else:\n",
    "            start = 1\n",
    "\n",
    "        # Start time\n",
    "        print(\"=\" * 30, \"\\nStart training...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.D.train()\n",
    "        self.G.train()\n",
    "\n",
    "        for step in range(start, self.total_step+1):\n",
    "\n",
    "            try:\n",
    "                real_imgs, _ = next(data_iter)\n",
    "            except:\n",
    "                data_iter = iter(self.data_loader)\n",
    "                real_imgs, _ = next(data_iter)\n",
    "                self.epoch += 1\n",
    "\n",
    "            real_imgs = real_imgs.to(self.device)\n",
    "\n",
    "            # ================ update D d_iters times ================ #\n",
    "            for i in range(self.d_iters):\n",
    "                \n",
    "                # ============== Genrate random Masks of target density from a uniform random distribution=================== #\n",
    "                \n",
    "                B,C,H,W = real_imgs.shape\n",
    "                img_masks = ((torch.rand(B,1,H,W)<=self.density)*1.0).to(device)\n",
    "                \n",
    "                # ============== Create Masked images ============== #\n",
    "                \n",
    "                masked_imgs = img_masks * real_imgs\n",
    "                \n",
    "                # ============== Generator - Image reconstruction ================= #\n",
    "                \n",
    "                fake_imgs = self.G(torch.cat((masked_imgs, img_masks, torch.normal(0.0,0.1,size=real_imgs.size()).to(device)), 1))              \n",
    "                \n",
    "                fake_input = torch.cat((fake_imgs, img_masks), 1)\n",
    "                real_input = torch.cat((real_imgs, img_masks), 1)\n",
    "\n",
    "\n",
    "        # ============== Calculate losses and update the networks =========== #                \n",
    "                    \n",
    "                d_fake = self.D(fake_input)\n",
    "                \n",
    "                # ============  Update the Generator_Net network (Wasserstein Generator loss + Image Reconstruction loss)\n",
    "\n",
    "                g_loss = 0.005*wass_g_loss(d_fake)\n",
    "                mse_loss = image_loss(fake_imgs, real_imgs)\n",
    "\n",
    "                g_combined_loss = g_loss + mse_loss # No density Loss\n",
    "                self.reset_grad()\n",
    "                g_combined_loss.backward()\n",
    "                self.g_optimizer.step()\n",
    "                \n",
    "                # ============ Update the Discriminator network (Wasserstein Discriminator loss)\n",
    "\n",
    "                d_fake = self.D(fake_input.detach())\n",
    "                d_real = self.D(real_input.detach())\n",
    "                d_loss = wass_d_loss(d_real, d_fake)\n",
    "                self.reset_grad()\n",
    "                d_loss.backward()\n",
    "                self.d_optimizer.step()\n",
    "\n",
    "                \n",
    "            # ==================== print & save part ==================== #\n",
    "            # Print out log info\n",
    "            if step % self.log_step == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "                start_time = time.time()\n",
    "                log_str = \"Epoch: [%d/%d], Step: [%d/%d], time: %s, d_real: %.4f, d_fake: %.4f, d_loss: %.4f, g_loss: %.4f, image_loss: %.4f\" % \\\n",
    "                    (self.epoch+1, self.total_epoch, step, self.total_step, elapsed, torch.mean(d_real), torch.mean(d_fake), d_loss, g_loss, mse_loss)\n",
    "\n",
    "                if self.use_tensorboard is True:\n",
    "                    write_log_no_mask(self.writer, log_str, step, torch.mean(d_real), torch.mean(d_fake), d_loss, mse_loss)\n",
    "                print(log_str)\n",
    "\n",
    "            # Save model\n",
    "            if step % self.model_save_step == 0:\n",
    "                torch.save(self.G.state_dict(),\n",
    "                           os.path.join(self.model_save_path, '{}_G.pth'.format(step)))\n",
    "                torch.save(self.D.state_dict(),\n",
    "                           os.path.join(self.model_save_path, '{}_D.pth'.format(step)))\n",
    "\n",
    "    def build_model(self):\n",
    "        '''Initialize Generator_Net + Discriminator along with their optimizers and load them onto relevant device'''\n",
    "\n",
    "        print(\"=\" * 30, '\\nBuild_model...')\n",
    "        self.G = Generator_Net(7).to(self.device)\n",
    "        self.D = Discriminator(4).to(self.device)\n",
    "\n",
    "        if self.parallel:\n",
    "            print('Use parallel...')\n",
    "            print('gpus:', os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "            self.G = nn.DataParallel(self.G, device_ids=self.gpus)\n",
    "            self.D = nn.DataParallel(self.D, device_ids=self.gpus)\n",
    "\n",
    "        self.select_opt_schr()\n",
    "        print(\"Model building done!\")\n",
    "\n",
    "    def build_tensorboard(self):\n",
    "        '''Initialize TensorboardX summary writer'''\n",
    "        \n",
    "        from tensorboardX import SummaryWriter\n",
    "        self.writer = SummaryWriter(log_dir=self.log_path)\n",
    "\n",
    "    def load_pretrained_model(self):\n",
    "        '''Load pretrained models of Generator_Net and Discriminator networks from self.model_save_path folder'''\n",
    "        \n",
    "        self.G.load_state_dict(torch.load(os.path.join(\n",
    "            self.model_save_path, '{}_G.pth'.format(self.pretrained_model))))\n",
    "        self.D.load_state_dict(torch.load(os.path.join(\n",
    "            self.model_save_path, '{}_D.pth'.format(self.pretrained_model))))\n",
    "        print('loaded trained models (step: {})..!'.format(self.pretrained_model))\n",
    "\n",
    "    def reset_grad(self):\n",
    "        '''Reset the gradients before backward propagation of losses'''\n",
    "        \n",
    "        self.d_optimizer.zero_grad()\n",
    "        self.g_optimizer.zero_grad()\n",
    "\n",
    "    def save_sample(self, data_iter):\n",
    "        '''Save relevant samples at the current step'''\n",
    "        \n",
    "        real_images, _ = next(data_iter)\n",
    "        save_image(real_images, os.path.join(self.sample_path, 'real.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9789eb",
   "metadata": {},
   "source": [
    "## 6. SET HYPERPARAMETERS FOR TRAINING\n",
    "\n",
    "The following are the hyperparameters and their values that were used across the training scenarios. Please make changes to the default values if necessary before starting the training.\n",
    "\n",
    "**Note:** Please change the paths to fit the OS file system. The following paths work on Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73b1db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Training setting\n",
    "        self.total_epoch=200                                       # Total number of training epochs\n",
    "        self.num_workers=2                                         # Number of dataloader workers\n",
    "        self.g_lr=5e-5                                             # Learning rate for Generator networks(Both Mask_Generator_Net and Generator_Net)\n",
    "        self.d_lr=5e-5                                             # Learning rate for the Discriminator network\n",
    "        self.batch_size=16                                         # Batch size for each epoch\n",
    "        self.d_iters=1                                             # Number of iterations for the Discriminator update step\n",
    "\n",
    "            # using pretrained\n",
    "        self.pretrained_model = False                              # Step number of the pretrained models to resume training from there\n",
    "\n",
    "            # Misc\n",
    "        self.parallel=True                                         # Use multiple GPUs if they are available \n",
    "        self.gpus=[\"0\"]                                            # List of GPUs to use\n",
    "        self.use_tensorboard=True                                  # Create tensorboardX logs\n",
    "\n",
    "            # Paths\n",
    "        self.data_dir = '.\\\\CelebA_HQ_facial_identity_dataset'     # Path of the root folder of dataset\n",
    "        self.log_path=\".\\\\output_dlvc\\\\logs\"                       # Path of the folder to save tensorboard logs\n",
    "        self.model_save_path=\".\\\\outputs_dlvc\\\\modelsZ\"            # Path of the folder to save trained models\n",
    "        self.sample_path=\".\\\\outputs\\\\samples\"                     # Path to save image samples. But code not written for it.   \n",
    "\n",
    "            # epoch size\n",
    "        self.log_epoch=1                                           # Create a tensorboardX log \n",
    "        self.sample_epoch=10                                       # Save an image sample with current models. But code not written for it.\n",
    "        self.model_save_epoch=1                                    # Save new models for each network being trained\n",
    "\n",
    "        self.density = 0.05                                        # Target density of pixels to be selected for each mask.\n",
    "        self.version = \"\"                                          # Version of the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "957e0e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CONFIG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23555ac4",
   "metadata": {},
   "source": [
    "## 7. PREPARE DATASET\n",
    "\n",
    "We use **CELEBA HQ Facial Identity dataset** in our experiments.\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"./Test_images/Celebrity1.jpg\" width=\"200\" /> </td>\n",
    "        <td> <img src=\"./Test_images/Celebrity2.jpg\" width=\"200\" /> </td>\n",
    "        <td> <img src=\"./Test_images/Celebrity3.jpg\" width=\"200\" /> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Please download the dataset from the following link: [CELEBA_HQ_facial_identity_dataset](https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/ES-jbCNC6mNHhCyR4Nl1QpYBlxVOJ5YiVerhDpzmoS9ezA?download=1) and unzip it. Please don't forget to provide the path of the root directory to the data_dir parameter of Config object.\n",
    "\n",
    "If you are using Linux, you could use the following commands:<br>\n",
    "1. wget https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/ES-jbCNC6mNHhCyR4Nl1QpYBlxVOJ5YiVerhDpzmoS9ezA?download=1 -O CelebA_HQ_facial_identity_dataset.zip\n",
    "2. unzip CelebA_HQ_facial_identity_dataset.zip -d ./CelebA_HQ_facial_identity_dataset\n",
    "\n",
    "Each image of the dataset has the resolution 1024x1024. For the experiments we resize them to 128x128. We mirror the images horizontally as a data augmentation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cde2f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(), # data augmentation\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(os.path.join(config.data_dir, 'train'), transforms_train)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec998aef",
   "metadata": {},
   "source": [
    "### CREATE FOLDERS FOR SAVING MODELS AND LOGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03887ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folder(path, version):\n",
    "    if not os.path.exists(os.path.join(path, version)):\n",
    "        os.makedirs(os.path.join(path, version))\n",
    "\n",
    "make_folder(config.model_save_path, config.version)\n",
    "make_folder(config.log_path, config.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e6d165",
   "metadata": {},
   "source": [
    "## 8. TRAINING\n",
    "\n",
    "1. Initialize the corresponding traininer object of the train scenarios. Please uncomment the relevant line of code\n",
    "2. Call the train function of the trainer.\n",
    "\n",
    "**NOTE:** The complete training was done on Google Colab. Therefore, the outputs aren't visible for the training scenario. To show that the code works I ran 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a4b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== \n",
      "Build_model...\n",
      "Model building done!\n",
      "Inside the trainer!\n",
      "Iterator created!\n",
      "steps per epoch: 267\n",
      "============================== \n",
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/2], Step: [267/534], time: 0:05:00.686826, d_real: -31.9983, d_fake: -27.4074, d_loss: -4.5909, g_loss: -0.1374, density_loss: 0.0000, image_loss: 0.1527\n"
     ]
    }
   ],
   "source": [
    "# config.total_epoch=200\n",
    "config.total_epoch=2\n",
    "\n",
    "# 1. To trian the Full model uncomment this and comment the subsequent one\n",
    "trainer = Trainer(train_dataloader, config) \n",
    "\n",
    "# 2. To train the model with randomly generated masks(No Mask network)\n",
    "# trainer = Trainer_NoMask(train_dataloader, config) \n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee84e1b",
   "metadata": {},
   "source": [
    "## FINAL MODELS\n",
    "\n",
    "The final models of both the training scenarios are saved in the DLVC_BestModels folder. Within it:\n",
    "1. **Full Model**: Both generator models saved in \"./DLVC_BestModels/Full_Training\" folder\n",
    "2. **No Mask N/w**: The Generator_Net model saved in \"./DLVC_BestModels/No_Mask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7867e60e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
